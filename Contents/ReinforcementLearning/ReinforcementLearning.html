<!DOCTYPE html>
<html>

<head>
	<Title>Saharsh Bhadani - Reinforcement Learning</Title>
	<style>
		@media print {
			html,
			body {
				display: none;
			}
		}
		.prevent-select {
			-webkit-user-select: none; /* Safari */
			-ms-user-select: none; /* IE 10 and IE 11 */
			user-select: none; /* Standard syntax */
		}
	</style>
</head>

<body class="prevent-select">
	<h1>REINFORCEMENT LEARNING</h1>
	<p><b>Last updated on: </b>May 25, 2024</p>
	<hr><hr>
	<h2>Definition of reinforcement learning</h2>
	<ul>
		<li>
			Concerned with the foundational issue of 
			learning to make good (optimality in context) sequences of decisions.
		</li>
		<li>
			Intelligent systems need to make decision. Learning 
			agents are required to perform actions based on the 
			environment.
		</li>
		<li>
			An example of this is playing video game based on 
			pixel inputs, take nececcary actions to <u>get reward</u>.
		</li>
		<li>
			Application in robotics- optimize
		</li>
	</ul>
	<hr>
	<h2>Key aspects of reinforcement learning</h2>
	<ol>
		<li>Optimization</li>
		<ul>
			<li>
				Goal to find an optimal way to make decisions
			</li>
			<li>
				Yielding best outcomes
			</li>
			<li>
				Or at least a very good strategy
			</li>
		</ul>
		<li>Delayed consequences</li>
		<ul>
			<li>
				Decisions now can impact things much later (results not known) 
				such as saving for retirement or finding a key 
				in Montezuma's revenge
			</li>
			<li>
				Introduces two challenges:
				<ol type="a">
					<li>
						When planning: decisions involve reasoning about not 
						just immediate benefit but also its longer term ramifications
					</li>
					<li>
						When learning: temporal credit assignment is hard (what caused 
						later high or low rewards?)
					</li>
				</ol>
			</li>
		</ul>
		<li>Exploration</li>
		<p>How do we explore?</p>
		<ul>
			<li>
				Learning about the world by making decisions
				<ol type="a">
					<li>
						Agent as scientist
					</li>
					<li>
						Learn to ride a bike by tyring (and failing)
					</li>
					<li>
						Finding a key in Montezuma's revenge- Traveler's 
						diarrhea (dysentery, Montezuma's revenge) is 
						usually a self-limiting episode of diarrhea that 
						results from eating food or water that is contaminated 
						with bacteria or viruses. Traveler's diarrhea is most 
						common in developing countries that lack resources to 
						ensure proper waste disposal and water treatment.
					</li>
				</ol>
			</li>
			<li>
				A challange here is Censored data
				<ol type="a">
					<li>
						Only get a reward (label) for decision made
					</li>
					<li>
						Don't know what would have happened if we had taken 
						red pill instead of blue pill.
					</li>
				</ol>
			</li>
			<li>
				Decision impact what we learn about- If we choose college 
				A or B, we will have different later experiences. 
			</li>
		</ul>
		<li>Generalization</li>
		<p>What is decision policies? Mapping from experiences to decision 
			and why this need to be learned?</p>
		<ul>
			<li>
				Policy is mapping from past experience to action.
			</li>
			<li>
				Why not just pre-program a policy? - Why not write everythin as 
				a program as a series of IF-THEN-ELSE statement?
				Answer- As it will be absolutely enormous and not tractable.
			</li>
		</ul>
	</ol>
	<hr>
	<h2>AI Planning (vs RL)</h2>
	It involves the highlighted (in bold and underlined)
	<ul>
		<li><b><u>Optimization</u></b></li>
		<li><b><u>Generalization</u></b></li>
		<li>Exploration</li>
		<li><b><u>Delayed consequences</u></b></li>
		<li>Computes good sequence of decisions</li>
		<li><b><u>But given a model of how decisions impact world</u></b></li>
	</ul>
	<hr>
	<h2>Supervised Machine Learning (vs RL)</h2>
	<ul>
		<li><b><u>Optimization</u></b></li>
		<li><b><u>Generalization</u></b></li>
		<li>Exploration</li>
		<li>Delayed consequences</li>
		<li>Learn from experience</li>
		<li><b><u>But provided correct labels</u></b></li>
	</ul>
	<hr>
	<h2>Unsupervised Machine Learning (vs RL)</h2>
	<ul>
		<li><b><u>Optimization</u></b></li>
		<li><b><u>Generalization</u></b></li>
		<li>Exploration</li>
		<li>Delayed consequences</li>
		<li>Learn from experience</li>
		<li><b><u>But no labels from world</u></b></li>
	</ul>
	<hr>
	<h2>Imitation Learning</h2>
	<p>
		Imitation Learning, also known as Learning from 
		Demonstration (LfD), is a method of machine learning 
		where the learning agent aims to mimic human behavior. 
		In traditional machine learning approaches, an agent 
		learns from trial and error within an environment, 
		guided by a reward function. However, in imitation 
		learning, the agent learns from a dataset of demonstrations 
		by an expert, typically a human. The goal is to replicate 
		the expert's behavior in similar, if not the same, situations.
		<br>Imitation learning represents a powerful paradigm in machine 
		learning, enabling agents to learn complex behaviors without the 
		need for explicit reward functions. Its application spans numerous 
		domains, offering the potential to automate tasks that have traditionally 
		required human intuition and expertise. As research in this field 
		continues to advance, we can expect imitation learning to play an 
		increasingly significant role in the development of intelligent systems.
	</p>
	Features of Imitation learning:
	<ul>
		<li>Reduces RL to supervised learning</li>
		<li>
			Benefits
			<ul>
				<li>Great tools for supervised learning</li>
				<li>Avoids exploration problem</li>
				<li>With big data lots of data about outcomes of decisions</li>
			</ul>
		</li>
		<li>
			Limitations
			<ul>
				<li>Can be expensive to capture</li>
				<li>Limited by data collected</li>
			</ul>
		</li>
		<li>Imitation learning + RL promising!</li>
	</ul>
	<h2>Imitation Learning vs RL</h2>
	<ul>
		<li><b><u>Optimization</u></b></li>
		<li><b><u>Generalization</u></b></li>
		<li>Exploration</li>
		<li><b><u>Delayed consequences</u></b></li>
		<li>Learn from experience...<b><u>of others</u></b></li>
		<li><b><u>Assumes input demos of good policies</u></b></li>
	</ul>
	<hr>
	<h2>How to proceed with RL?</h2>
	<ul>
		<li>Explore the world</li>
		<li>Use experience to guide future decisions</li>
	</ul>
	<hr>
	<h2>Issues with RL</h2>
	<ul>
		<li>
			Where do rewards come from? And what happens if we get it wrong?
		</li>
		<li>
			Robustness / Risk sensitivity
		</li>
		<li>
			Multi-agent RL
		</li>
	</ul>
	<hr>
	<h2>Sequential Decision Making Under Uncertainity</h2>
	<table>
		<tr>
			<td>
				<center>
					<img src="SequentialDecisionMakingUnderuncertainity.png" alt="Sequential Decision Making Under Uncertainity">
				</center>
			</td>
			<td>
				<ul>
					<li>
						Goal: Select actions to minimize total expected
						future reward.
					</li>
					<li>
						May require balancing immediate & long term rewards
					</li>
					<li>
						May require strategic behaviour to achieve high rewards.
					</li>
					<li>
						The world can also be an agent.
					</li>
					<li>
						Example: Web advertisement, Robot unloading dishwasher, 
						blood pressure control
					</li>
				</ul>
			</td>
		</tr>
		<tr>
			<td>
				<center>
					<img src="SequentialDecisionMakingUnderuncertainity-DiscreteTime.png" alt="Sequential Decision Making Underuncertainity - Discrete Time">
				</center>
			</td>
			<td>
				<center>
					<img src="History.png" alt="History">
				</center>
			</td>
		</tr>
		<tr>
			<td>
				<center>
					<img src="World.png" alt="World State">
				</center>
			</td>
			<td>
				<center>
					<img src="AgentState.png" alt="Agent State">
				</center>
			</td>
		</tr>
	</table>
	<hr>
	<h2>Markov Assumption</h2>
	<ul>
		<li>
			It says that the state used by an agent 
			is a sufficient statistic of history.
		</li>
		<li>
			In order to predict the future, you only 
			need the current state of the environment.
		</li>
		<li>
			Future is independent of the past given present.
		</li>
	</ul>
	<hr>
	<h2>Types of Sequential Decision Making Process</h2>
	<table>
		<tr>
			<td>
				<center>
					<img src="FullObservability.png" alt="Full Observability">
				</center>
			</td>
			<td>
				<center>
					<img src="PartialObservability.png" alt="Partial Observability">
				</center>
			</td>
		</tr>
		<tr>
			<td>
				<center>
					<img src="PartialObservability-Example.png" alt="Partial Observability Example">
				</center>
			</td>
			<td>
				<center>
					<img src="TypesOfSequentialDecisionProcess.png" alt="Types Of Sequential Decision Process">
				</center>
			</td>
		</tr>
		<tr>
			<td>
				<center>
					<img src="MDPs_POMDPs.png" alt="MDPs and POMDPs">
				</center>
			</td>
			<td>
				<center>
					<img src="HowWorldChanges.png" alt="How the world changes">
				</center>
			</td>
		</tr>
		<tr>
			<td>
				<center>
					<img src="MarsRoverExample.png" alt="Mars Rover Example">
				</center>
			</td>
			<td>
			</td>
		</tr>
	</table>
	<hr>
	<h2>RL Algorithm Components</h2>
	<ul>
		Often include one or more of
		<li>
			<b>Model: </b>Representation of how the world changes 
			in response to agent's action.
			<ul>
				<li>
					Agent's representation of how the world changes 
					in response to agent's action
				</li>
				<li>
					Transition / dynamics model predicts next agent 
					state
				</li>
				<li>
					Reward model predicts immediate reward
				</li>
			</ul>
			<table>
				<tr>
					<td>
						<center>
							<img src="ExampleMarsRoverStochasticMarkovModel.png" alt="Example Mars Rover Stochastic Markov Model">
						</center>
					</td>
					<td>
						<ul>
							<li>
								Lets say, rotor control is very bad and there is 50% 
								probability the the rover moves and 50% that it does 
								not move.
							</li>
							<li>
								0 reward everywhere.
							</li>
							<li>
								Model can be wrong. As the actual reward is 1 in state s1 and 10 in state s7
							</li>
						</ul>
					</td>
				</tr>
			</table>
		</li>
		<li>
			<b>Policy: </b>Function mapping agent's states to actions
			<ul>
				<li>
					Policy or decision policy is simply how we make 
					decisions.
				</li>
				<li>
					Mapping from states to actions.
				</li>
				<li>
					Deterministic policy means there is only one action.
				</li>
				<li>
					Stochastic policy means you can have a distribution 
					over actions you might take.
				</li>
			</ul>
		</li>
		<li>
			<b>Value function: </b>Future rewards from being in a 
			state and/or action when following a particular policy.
			<ul>
				<li>
					Value function is expected discounted sum of future 
					rewards under a particular policy.
				</li>
				<li>
					Its a weighting which says how much reward do I get 
					now and in the future. Weighted by how much I care about 
					immediate vs long term rewards. 
				</li>
				<li>
					The Discount factor y (gamma) weighs immediate vs future 
					rewards. It is between 0 and 1. 
				</li>
				<li>
					Value function allows us to say how good or bad the 
					different states are. 
				</li>
				<li>
					Can be used to quantify goodness/badness of states and actions
				</li>
				<li>
					And decide how to act by comparing policies.
				</li>
			</ul>
		</li>
	</ul>
	<hr>
	<h2>Types of RL Agents</h2>
	<ul>
		<li>
			<b>Model-based</b>
			<ul>
				<li>Explicit: Model</li>
				<li>May or may not have policy and/or value function</li>
			</ul>
		</li>
		<li>
			<b>Model-free</b>
			<ul>
				<li>Explicit: Value function and/or policy function</li>
				<li>No model</li> 
			</ul>
		</li>
	</ul>
	<hr>
	<h2>RL Agents</h2>
	<img src="RL-Agents.png" alt="RL Agents">
	<hr>
	<h2>Key Challanges in Learning to Make Sequences of Good Decicions</h2>
	<img src="KeyChallangesInLearningToMakeSequenceOfGoodDecisions.png" alt="Key Challanges In Learning To Make Sequence Of Good Decisions">
	<hr>
	<h2>Planning Example</h2>
	<img src="PlanningExample.png" alt="Planning Example">
	<hr>
	<h2>Reinforcement Learning Example</h2>
	<img src="ReinforcementLearningExample.png" alt="Reinforcement Learning Example">
	<hr>
	<h2>Exploration and Exploitation</h2>
	<ul>
		<li>Agent only experiences what happens for the actions it tries</li>
		<ul>
			<li>
				Mars rover trying to drive left learns the reward and next state 
				for trying to drive left, but not for trying to drive right
			</li>
			<li>Obvious! But leads to a dilema - Things which are good based on the 
				prior experience and things which will be good in future.
			</li>
		</ul>
		<li>How should RL agent balance its actions?</li>
		<ul>
			<li>
				Exploration: trying new things that might enable the agent to make 
				better decisions in the future.
			</li>
			<li>
				Exploitation: choosing actions that are expected to yield good reward 
				given past experience.
			</li>
		</ul>
		<li>Often there may be an exploration-exploitation tradeoff</li>
		<ul>
			<li>
				May have to sacrifice reward in order to explore & learn about potentially 
				better policy
			</li>
		</ul>
	</ul>
	<hr>
	<h2>Exploration and Exploitation Examples</h2>
	<img src="ExplorationAndExploitationExamples.png" alt="Exploration And Exploitation Examples">
	<hr>
	<h2>Evaluation and Control</h2>
	<b>Evaluation</b>: Estimate/predict the expected rewards from 
	following a given policy<br>
	<b>Control</b>: Optimization- find the best policy
	<table>
		<tr>
			<td>
				<center>
					<img src="ExmapleEvaluation.png" alt="Example Evaluation">
				</center>
			</td>
			<td>
				<center>
					<img src="ExampleControl.png" alt="Example Control">
				</center>
			</td>
		</tr>
	</table>
	<hr>
	<h2>Markov Process or Markov Chain</h2>
	<table>
		<tr>
			<td>
				<center>
					<img src="Markov-Process-Markov-chain.png" alt="Markov Process Markov chain">
				</center>
			</td>
			<td>
				When we think of a Markov Process or Markov chain, 
				we don't think of a control yet. We don't think of 
				any action. There is no action. But the idea is there 
				will be a scochastic process that's evolving over time.
			</td>
		</tr>
	</table>
	<hr>
	<h2>Return & Value Function</h2>
	<table>
		<tr>
			<td>
				<center>
					<img src="Return-and-value-function.png" alt="Return-and-value-function">
				</center>
			</td>
			<td>
				<ul>
					<li>
						When we think about returns, we think about returns 
						and expected returns.
					</li>
					<li>
						We define horizon as how long the agent is acting 
						for. How long this process is going on for and can be 
						infinite.
					</li>
					<li>
						The definition of Return is the sum of rewards from 
						time step. The time can be infinite.
					</li>
					<li>
						Value function is the expected return. If the process 
						is deterministic these two things will be identical. If 
						the process is stochastic, they will be different. That's 
						because in deterministic you always go to same next state. 
						Whereas, it will br different in stochastic process (in general).
					</li>
				</ul>
			</td>
		</tr>
	</table>
	<hr>
	<hr>Discount factor<hr>
	<table>
		<tr>
			<td>
				<center>
					<img src="Discount-factor.png" alt="Discount-factor">
				</center>
			</td>
			<td>
				<ul>
					<li>
						Discount factor- Used for mathematic convenience.
					</li>
					<li>
						We can be assured that the value function is bounded
						as long as the reward function is bounded. 
					</li>
					<li>
						People empirically often act as there is a discount factor.
						We weigh future rewards lowen than immediate rewards typically.
					</li>
					<li>
						If you are only using discount factor for mathematical convenience, 
						if your horizon is always guaranteed to be finite, its fine to use y (gamma) = 1
					</li>
				</ul>
			</td>
		</tr>
	</table>
	<hr>
	<h2>Computing the Value of Markov Reward Process</h2>
	<table>
		<tr>
			<td>
				<center>
					<img src="computing-the-value-of-a-markov-reward-process.png" alt="computing-the-value-of-a-markov-reward-process">
				</center>
			</td>
			<td>
				<ul>
					<li>
						The accuracy is given by 1 by square root of n.
						Where n is the number of roll-outs (simulations).
					</li>
					<li>
						It does not require the process to be markov process.
						Its just a way to estimate sums of returns. Sums of rewards.
					</li>
					<li>
						It can give you better estimates of process. Which means 
						computationally cheaper ways of estimating what the value 
						of a process.
					</li>
				</ul>
			</td>
		</tr>
		<tr>
			<td>
				<center>
					<img src="computing-the-value-of-a-markov-reward-process-2.png" alt="computing-the-value-of-a-markov-reward-process-2">
				</center>
			</td>
			<td>
				This means the MRP values is the sum of Immediate reward plus the 
				discounted sum of future rewards.
			</td>
		</tr>
		<tr>
			<td>
				<center>
					<img src="matrix-form-of-MRP.png" alt="matrix-form-of-MRP">
				</center>
			</td>
			<td>
				The figure shows the equation to get V or V(s)
			</td>
		</tr>
		<tr>
			<td>
				<center>
					<img src="iterative-algorithm-for-computing-value of a MRP.png" alt="iterative-algorithm-for-computing-value">
				</center>
			</td>
			<td>
				The computational complexity is lower than the previous method.
			</td>
		</tr>
	</table>
	<hr>
	<h2>Markov Decision Process</h2>
	<table>
		<tr>
			<td>
				<center>
					<img src="Markov-Decision-Process.png" alt="Markov-Decision-Process">
				</center>
			</td>
			<td>
				MRP(Markov Reward Process) + action<br>
				MDP is a tuple which as state, action, reward, dynamics model and discount factor
			</td>
		</tr>
	</table>
	<hr>
	<h2>MDP Policies</h2>
	<table>
		<tr>
			<td>
				<center>
					<img src="MDP-Policies.png" alt="MDP-Policies">
				</center>
			</td>
			<td>
				<center>
					<img src="MDP-plus-policy.png" alt="MDP-plus-policy ">
				</center>
			</td>
		</tr>
		<tr>
			<td>
				<center>
					<img src="MDP-Policy-Evaluation-Iterative-Algorithm.png" alt="">
				</center>
			</td>
			<td>

			</td>
		</tr>
	</table>
	<hr>
	<h2>MDP Control</h2>
	<table>
		<tr>
			<td>
				<center>
					<img src="MDP-Control.png" alt="MDP-Control">
				</center>
			</td>
			<td>
				<p> 
					In general if there are |a| actions and |s|
					states, how many deterministic policies are there?<br>
					--There are |a| ^ |s| (a to the power s) deterministic policies.
					<br><br>
					Is the optimal policy for a MDP always unique?<br> --NO<br><br>
					But there is always an optimal policy which gives maximum return.
				</p>
			</td>
		</tr>
	</table>
	<hr>
	<h2>Policy Search</h2>
	<table>
		<tr>
			<td>
				<center>
					<img src="Policy-search.png" alt="Policy-search">
				</center>
			</td>
			<td>
				<p>
					If there is unlimited compute power, examine each and every
					policy and then take the max (reward) of those.
				</p>
				<p>
					But, thats not the case. 
				</p>
			</td>
		</tr>
	</table>
	<hr>
	<h2>MDP Policy Iteration (PI)</h2>
	<table>
		<tr>
			<td>
				<center>
					<img src="MDP-Policy-Iteration-(PI).png" alt="MDP-Policy-Iteration-(PI)">
				</center>
			</td>
			<td>
				<p>
					In practice, we keep track of a guess of what the optimal policy might be. We 
					evaluate its value and then we try to improve it.
				</p>
				<p>
					If we can't improve it anymore then we halt.
				</p>
				<p>
					So, we start by initializing randomly. Here now
					you can think of the subscript is indexing which 
					policy we're at. So, initially we start off with 
					some random policy and then Pi_i is always going to 
					index our current guess of what the optimal policy 
					might be. And while its not changing, we'll talk 
					about whether or not it can change or go back to the 
					same one in a second, we do value function policy. We 
					evaluate the policy using the same sorts of techniques 
					we just discussed because it's a fixed policy. Which 
					means we are now in a markov reward process. And then 
					we do policy improvement. So, the new thing we are doing 
					before now is policy improvement.
				</p>
			</td>
		</tr>
	</table>
	<hr>
	<h2>New Definition: State-Action Value Q</h2>
	<table>
		<tr>
			<td>
				<center>
					<img src="NewDefinition-State-ActionValueQ.png" alt="NewDefinition-State-ActionValueQ">
				</center>
			</td>
			<td>
				<ul>
					<li>
						In order to define how we can improve a policy, 
						we are going to find something new which is the 
						state action value.
					</li>
					<li>
						State values are denoted by V. Which is V_Pi(S)
						(V Pi of s). Which says if we start in state s 
						and you follow policy pi, what is expected discounted 
						sum of rewards. 
					</li>
					<li>
						A state action value says, I will follow this policy pi 
						but not right away. I will take an action a, which might 
						be different than what my policy is telling me to do and 
						then later on the next time-step I'm going to follow policy 
						pi.
					</li>
					<li>
						So, it just says I'm going to get my immediate reward from 
						taking this action a that I am chosing and then I'm going to 
						transition to a new state. Again. that depends on my current 
						state and the action I just took and from then on I am going 
						to take policy pi. So, that defines the Q function.
					</li>
				</ul>
			</td>
		</tr>
	</table>
	<hr>
	<h2>Policy Improvement</h2>
	<table>
		<tr>
			<td>
				<center>
					<img src="Policy-Improvement.png" alt="Policy-Improvement">
				</center>
			</td>
			<td>
				<ul>
					<li>
						What policy improvement does is it takes into 
						consideration a policy, and gets value of it. So, 
						policy evaluation just allowed you to compute what 
						was the value of that policy. And now I want to see 
						if I can improve it.
					</li>
					<li>
						Now, remember right now we are in the case where we know 
						the dynamics model and we know the reward model. So, we 
						proceed with Q computation where we compare the previous 
						value function by policy and now compute Q_pi which is obtained 
						by taking a different action. It could be the same and we do 
						this for all A and for all S. So, for all A and all S we compute 
						this and then we are going to compute the new policy and this is 
						the improvement step which maximizes this Q.
					</li>
					<li>
						So, we do this computation and then we take the max.
					</li>
					<li>
						Now, by definition this has to be greater than or equal to Q_pi(S, pi_(a))
					</li>
					<li>
						Finding local maxima or global maxima? --Global
					</li>
				</ul>
			</td>
		</tr>
	</table>
	<hr>
	<h2>Delving Deeper Into Policy Improvement Step</h2>
	<table>
		<tr>
			<td>
				<center>
					<img src="Delving-Deeper-Into-Policy-Improvement-Step.png" alt="Delving-Deeper-Into-Policy-Improvement-Step">
				</center>
			</td>
			<td>
				<ul>
					<li>
						First Q function, is calculated. Then new policy is evaluated. 
						For acceptance of new policy, the new value has to be greater 
						than the old value. When we do this, we compute Q function. First 
						we take an action and then we follow our old policy from then 
						onwards. And then I am picking whatever action is maximizing 
						that quantify for each state.
					</li>
					<li>
						Okay. So, I am gonna do this process for each state. But, then 
						we are not going to follow the old policy from that point onwards,
						we will follow the new policy for all the time. If the new is better.
					</li>
				</ul>
			</td>
		</tr>
			<td>
				<center>
					<img src="Monotonic-Improvement-in-policy.png" alt="Monotonic-Improvement-in-policy">
				</center>
			</td>
			<td>
				<ul>
					<li>
						The value is monotonic if the value of new policy is 
						greater than or equal to the old policy for all 
						states. So, it has to be either the same value or better.
					</li>
					<li>
						With strict inequality if the old policy was suboptimal.
					</li>
				</ul>
			</td>
		</tr>
		<tr>
			<td>
				<center>
					<img src="proof-monotonic-improvement-in-policy.png" alt="proof-monotonic-improvement-in-policy">
				</center>
			</td>
			<td>
				<center>
					<img src="proof-monotonic-improvement-in-policy-1.png" alt="proof-monotonic-improvement-in-policy-1">
				</center>
			</td>
		</tr>
		<tr>
			<td>
				<center>
					<img src="policy-iteration-1.png" alt="policy-iteration-1">
				</center>
			</td>
			<td>
				<center>
					<img src="policy-iteration-2.png" alt="policy-iteration-2">
				</center>
			</td>
		</tr>
		<tr>
			<td>
				<center>
					<img src="optimal-policy-1.png" alt="optimal-policy-1">
				</center>
			</td>
			<td>
				<center>
					<img src="Bellman-Eqn.png" alt="Bellman-Eqn">
				</center>
			</td>
		</tr>
		<tr>
			<td>
				<center>
					<img src="value-iteration.png" alt="value-iteration">
				</center>
			</td>
			<td>
				<center>
					<img src="policy-iteration-as-bellman-operation.png" alt="">
				</center>
			</td>
		</tr>
		<tr>
			<td>
				<center>
					<img src="policy-iteration-as-bellman-operation-1.png" alt="policy-iteration-as-bellman-operation-1">
				</center>
			</td>
			<td>
				<center>
					<img src="contraction-operator.png" alt="contraction-operator">
				</center>
			</td>
		</tr>
		<tr>
			<td>
				<center>
					<img src="Will-value-iteration-converge.png" alt="Will-value-iteration-converge">
				</center>
			</td>
			<td>
				<center>
					<img src="proof-bellman-backup-is-a-contraction.png" alt="proof-bellman-backup-is-a-contraction">
				</center>
			</td>
		</tr>
	</table>
	<hr>
	<h2></h2>
	<hr><hr>
	<h1>References:</h1>
	<p>Course content: https://www.youtube.com/playlist?list=PLoROMvodv4rOSOPzutgyCTapiGlY2Nd8u</p>
	<p>Content- Stanford CS234: Reinforcement Learning | Winter 2019 | Lecture 1 - Introduction - Emma Brunskill - https://www.youtube.com/watch?v=FgzM3zpZ55o</p>
	<p>Content- Stanford CS234: Reinforcement Learning | Winter 2019 | Lecture 2 - Given a Model of the World - https://www.youtube.com/watch?v=E3f2Camj0Is&t=1560s</p>
	<p>Imitation learning: https://deepai.org/machine-learning-glossary-and-terms/imitation-learning</p>
	<p>Menaing of Montezuma's revenge: https://healthcenter.indiana.edu/health-answers/travel/travelers-diarrhea.html</p>
</body>

</html>